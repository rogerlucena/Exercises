
--- Some of my notes, useful links and observations ---

-- To compile (command line)
g++ -std=c++17 main.cpp -o main; ./main

-- Notation for titles inside ./Problems folder:
"__" stands for good problems with interesting / key ideas to remember
"_" stands for more interesting / difficult problems, compared to the usual (but not necessarily with key ideas to remember)


-- Tree traversals (Inorder, Preorder and Posorder)
http://www.geeksforgeeks.org/tree-traversals-inorder-preorder-and-postorder/

E.g.: Inorder:
1) left child
2) this.data
3) right child
(The other traversals just change the position of this.data)

Obs: Preorder to copy the tree (polish_notation too), Posorder to delete, Inorder to get int numbers in ascending order from a BST

- Construct Tree from given Inorder and Preorder traversals:
http://www.geeksforgeeks.org/construct-tree-from-given-inorder-and-preorder-traversal/

- Construct a Binary Search Tree from given postorder
http://www.geeksforgeeks.org/construct-a-binary-search-tree-from-given-postorder/

-- STL Containers Complexities:
https://www.cs.northwestern.edu/~riesbeck/programming/c++/stl-summary.html

-- Sorting:

- Stable means that i < j, A[i] == A[j], than pi(i) < pi(j), where "pi" is the map for the new positions (after sorting).
- Merge and Insertion sort are stable, Quick and Heap are not:
https://www.geeksforgeeks.org/stability-in-sorting-algorithms/

- Quick Sort (cache friendly, not stable, in place, worst case is O(nÂ²), but avoided if we choose a good pivot (near the median) - random):
https://www.geeksforgeeks.org/quick-sort/

- Merge Sort (stable, additional O(n) in space but better for big data in different databases. For linked lists, where insertion is O(1), no need of additional space):
https://www.geeksforgeeks.org/merge-sort/

- Counting Sort (auxiliar array - 0 to 100 ages, O(n) time) and Radix Sort (O(kn) where k = log(max_el))
They are better explained on the Cracking the Coding Interview.
Also, Radix sort: https://www.geeksforgeeks.org/radix-sort/

-- Interesting way of calculating binomial coeffs:
https://www.geeksforgeeks.org/binomial-coefficient-dp-9/

-- Complexity:
n < nlog(n)
n^x < n^y if x < y
log(n) < n
n^x < c^n where c is a constant (polynomials will eventually grow slower than exponentials)
c^n < n! < n^n where c is a constant

-- For complexity with recursions, see "Master method":
https://www.geeksforgeeks.org/analysis-algorithm-set-4-master-method-solving-recurrences/

-- Tail-recursive
A function is tail-recursive if it ends by returning the value of the recursive call.
See tail-recursive sum: https://www.baeldung.com/cs/tail-vs-non-tail-recursion#1-tail-recursive-sum

-- Quick-Sort with Tail-Recursive Optimization (TRO or TCO)
http://mypathtothe4.blogspot.com/2013/02/lesson-2-variations-on-quicksort-tail.html ("How this works is that instead of 'rigid' branches as in the original Quicksort, the new branches can swing into different positions to cover every path")
Support: https://www.geeksforgeeks.org/quicksort-tail-call-optimization-reducing-worst-case-space-log-n/ 
(for this last link: taking the side with less elements first we guarantee it has maximun n/2 elements, then the recursion tree has a maximun logn in height - additional space in the stack =D)

-- Two pointers idea (very common, tricks):
Start with the brute force and then analyze if you can optimize it - discard some iterations that will not improve the result
https://www.interviewbit.com/tutorial/two-pointers/

-- Topological Sorting (or Ordering) for DAGs:
- Using "inDegree" array and a queue "q" - add vertex to "q" only if its current inDegree is zero (never happens for 
vertices in a cycle): https://www.geeksforgeeks.org/topological-sorting-indegree-based-solution/
(if "visitedCount != V" we have cycle - question at VTEX and "__course_schedule" problem)
- Using DFS and a stack (add vertex to it only after going through all the neighbors) - only to find the TO 
if you already know it is a DAG: https://www.geeksforgeeks.org/topological-sorting/ 
Obs: The first vertex in topological sorting is always a vertex with in-degree as 0 (a vertex with no in-coming edges).
Obs: A DAG G has at least one vertex with in-degree 0 and one vertex with out-degree 0.
Obs: Max number of edges in a DAG is O(n^2), think about the bipartite graph Kn,n with all edges oriented to 
the right (DAH with 2n nodes and n^2 edges), or see https://stackoverflow.com/questions/11699095/how-many-edges-can-there-be-in-a-dag ((n-1) + (n-2) + ... + 0).

-- Applications of DFS:
O(V+E) - remember algorithm
1. Detecting a graph's cycle: A graph has a cycle if and only if a back edge is visible during DFS. As a result,
you may run DFS on the graph to look for rear edges (or topo sort with indegree and visitedCount above).
2. Path Finding: The DFS algorithm can be customized to discover a path between two specified vertices, a and b.
    a. Use s as the start vertex in DFS(G, s).
    b. Keep track of the path between the start vertex and the current vertex using a stack S.
    c. Return the path as the contents of the stack as soon as destination vertex c is encountered.
3. To determine if a graph is bipartite: You can use either BFS or DFS to color a new vertex opposite its parents when you first discover it.
And check that each other edge does not connect two vertices of the same color. A connected component's first vertex can be either red or black.

-- BFS
O(V+E) as well
1. To find a path with BFS, you can keep track of the parents in a hash map till you find target, and then backtrace
after that and reverse vector - reverse(v.begin(), v.end()) (https://stackoverflow.com/questions/8922060/how-to-trace-the-path-in-a-breadth-first-search).

-- Process vs Thread
Process is a program under execution (program counter, process stack, registers, program code ...)
Thread is a lightweight process that can be managed independently by a scheduler.
Process contains threads.
Processes are independent (need more work to communicate), threads inside the same Process may share memory/code and are dependant.
(https://www.tutorialspoint.com/difference-between-process-and-thread)
Thread is an execution unit (executing an instruction) that is part of a process. A process can have multiple threads, all executing at the same time.
It is a unit of execution in concurrent programming. A thread is lightweight and can be managed independently by a scheduler.
It helps you to improve the application performance using concurrency or parallelism (the later if you have multiple cores).
(very good site: https://www.guru99.com/difference-between-process-and-thread.html)
(good video: https://youtu.be/fxMOi7BlTyM)

-- Semaphores
Basically and int shared between threads.
Controls the number of threads that can access a resource at the same time.
Two operations: wait (until s > 0, then --s) and signal (++s), both atomic.
Critical zone of the code = area between Wait and Signal.
If s = 4 and 4 threads enter their critical zones, a 5th thread will need to wait until one of the others Signal (end of its critical zone) to it is unlocked.
(very good: https://www.geeksforgeeks.org/semaphores-in-process-synchronization/)

-- Core vs Thread
A CPU core is a physical hardware component.
Thread is a virtual component that is used to manage the tasks.
(https://www.temok.com/blog/cores-vs-threads/, https://www.guru99.com/cpu-core-multicore-thread.html)
"Basically consider that the cores are your mouth and threads are your hand. The process done by the CPU be eating your food."

-- Deadlock
Deadlock describes a situation where two or more threads are blocked forever, waiting for each other.
Deadlock occurs when multiple threads need the same locks but obtain them in different order.
Thread 1: Holding lock 1...
Thread 2: Holding lock 2...
Thread 1: Waiting for lock 2...
Thread 2: Waiting for lock 1...
Solution: change the order on which you get locks: make both critical zones get locks 1 -> 2 in this order!
- How to handle it?
Eliminate Circular Wait: each resource will be assigned with a numerical number. A process can request the resources increasing/decreasing. order of numbering.
Eliminate Hold and Wait: allocate all required resources to the process before the start of its execution, but it may lead to low device utilization.
Eliminate No Preemption: preempt resources from the process when resources required by other high priority processes.
(https://www.geeksforgeeks.org/deadlock-prevention/)

-- Hashmap implementation and collisions
Implemented over an array.
Hash function to map key to index in array.
Linked list for each position in the array, Get is amortized to O(1).
If collision, add back to the Linked List for that index and keep the pre-hash key there for comparison as well.
Other way to handle collisions: keep going in the array till finding next available position, add there together with the pre-hash key too.
    (When Get is called will go through the array, if empty skips, if not empty checks pre-hash key before returning Value).
Some implementations also use a balanced tree instead of a linked list to improve worst case Get from O(N) to O(log N).

-- Data structure bizuradas menos macaco
Problema: priority queue com update eficiente.
min-heap de {int val; int id;} onde quer update val de um certo id
Exemplo: dijkstra
Solucao:
set<pair<int, int>> pq; // pair (priority, id)
vector<int> id_to_val; // assumindo id (0, ..., n-1), se nao usa hash map
Menor value em O(1): usa *pq.begin();
Update de um value em O(log n):
1) pq.erase(make_pair(id_to_val[id], id)); // here the val is the same as priority
2) id_to_val[id] = new_val;
3) pq.insert(make_pair(id_to_val[id], id));
Remember, could also customize the comparator of the set (more in remember_set_and_map.cpp):
eg: set<string, decltype(cmp)> s(cmp);
// Note: you can implement a pq using a set, but seems to be slower in average (since you can also iterate it all in sorted manner, see https://leetcode.com/problems/top-k-frequent-words/).
// do this only when you want to have id_to_val hash map and update val for given id, see My Notes.

-- Complexidade em tempo da PD
Ã o tamanho do cache * time de calcular uma cells
eg: se n * m o tamanho de `memo`, e cada vez vocÃª itera n, complexidade final Ã© O(m * n^2)

-- Virtual and pure virtual methods in C++, Override keyword
Virtual methods are marked at the parent class so, if a pointer of parent class type is used to reference a child class object,
the method of the child class is called (while for non-virtual methods the base class method is called - this is called runtime polymorphism).
(good: https://www.geeksforgeeks.org/virtual-function-cpp/)
"Override" keyword makes sure that a base class has an identical prototype in one of its virtual functions: https://www.fluentcpp.com/2020/02/21/virtual-final-and-override-in-cpp/
(showing the intention to the compiler that you want to override a virtual of the base, avoiding bugs of typos or different signatures with const etc)
A pure virtual method is specified by placing "= 0" in its declaration (cannot instantiate an object, then), eg: 
class Box {
  public:
    // pure virtual function
    virtual double getVolume() = 0;
  private:
    double length; // Length of a box
};
Obs: public and private keywords use half of the normal tab space (from Google C++ guide).
Obs: pure virtual methods are sometimes also called abstract methods.
(to clarify more pure virtual vs virtual: https://www.geeksforgeeks.org/difference-between-virtual-function-and-pure-virtual-function-in-c/)
(it is still valid to use Override for a child class method referencing a pure virtual parent class method: https://stackoverflow.com/questions/46446652/is-there-any-point-in-using-override-when-overriding-a-pure-virtual-function)

-- Abstract classes in C++ (same as interface in C++)
Has at least one pure virtual method (and then cannot be instantiated).
Describes the behavior or capabilities of a C++ class without committing to a particular implementation of that class.
Just for other classes to inherit from it following the same signature.
(https://www.tutorialspoint.com/cplusplus/cpp_interfaces.htm)

-- Naming
Mantenha coerÃªncia, C++ style guide:
MyMethodName, MyClassName, my_local_var
(Pascal case for methods and classes, underlines for variables)

-- Note to access unordered_map: g[node] or g.at[node]
If g[node] may receive an error if g was passed by const ref, because [] syntax also allows to change g.
g.at(node) is only read, then it works for const ref, but throws an error if node not present in g, has to deal with this edge case in a separate if.

-- CPUs vs GPUs
GPUs are faster for parallelization, image processing, matrices multiplications.
- When CPUs are better than GPUs?
Due to its architecture, a CPU is ideal for serial instruction processing, while a GPU is designed for parallel instruction processing.
Some computing tasks (most) canât be broken down to use lots and lots of cores operating in parallel. They need one or two cores operating fast, CPU is perfect for that.
Analogy: tens of tea pickers are faster than one very able one, while an experienced montain climber climbs much faster than many unexceptional others (different tasks, cannot parallelize the later).
(image processing or 3d rendering are highly parallelizable, each thread gets its own bit of screen to draw in parallel)
CPUs are more flexible, we are trading flexibility for performance (same happens on GPU -> TPU).
GPUs are very bad at doing floating point math, and so are not useful for rendering or simulations that require a lot of floating-point operations.
GPUs are also not very good at doing lots of small tasks serially, while CPUs are great at this.
- Why GPUs faster?
GPUs have hundreds OR thousands of weaker smaller cores, great for parallelization, where 32 threads work on the same
instruction (generally 16 Streaming Multiprocessors of 32 cores each = ~512 cores) as against a single thread in a CPU.
(SIMT - Single Instruction Multiple Threads for GPU)
Processing units organized like in a matrix fashion, immediate to access memory of neighboring units.
Using actual genuine thread rotation, a GPU launches instructions every time from different threads. Unlike a GPU, a CPU uses out-of-order execution.
All modern GPUs come with shared memory, which is several times faster than the bandwidth of a CPUâs L1 cache. Itâs designed explicitly for algorithms with a high degree of locality.
- GPUs for ML 
Most CPUs are multi-core processors, operating with an MIMD architecture. In contrast, GPUs use a SIMD architecture. This difference
makes GPUs well-suited to deep learning processes which require the same process to be performed for numerous data items. Distribution of training processes.
Both CPUs and GPUs have ALUs (Arithmetic Logic Units) as building blocks.
- Systolic array (organization of processing units in a matrix-like fashion)
Systolic arrays are hardware structures built for fast and efficient operation of regular algorithms that perform the same task with different data at different time instants.
Systolic arrays replace a pipeline structure with an array of processing elements that can be programmed to perform a common operation.
TPUs have them, some GPUs may have as well (not all it seems).
Neighbor cells can directly access memory of the neighbors.
- CPU
1. CPU normally has only 4 (few) powerful cores.
2. A CPU architecture allows each physical CPU core to execute two threads on two virtual cores such that an individual thread executes the instructions independently.
CPUs are designed to run almost any calculation; they're general-purpose computers.
- GPU
1. GPU has hundreds OR thousands of weak and smaller cores.
2. GPU uses single instruction, multiple threads (SIMT) architecture, where 32 (generally) threads work on the same instruction as against a single thread in a CPU.
GPUs use SPMD.
(https://www.e2enetworks.com/blog/why-gpu-can-process-image-much-faster-than-cpu)

-- TPUs
"Custom build ASIC to accelerate ML and DL projects".
Remember ASICs (Application-Specific Integrated Circuits).
- The heart of the TPU: A systolic array (very different architecture from GPUs and CPUs).
- Systolic because it likes the data goes in waves (like heart) accross the processing units (the same input for a register is passed through
neighbor ALUs without the need to be saved in a register again, only at the end, neighbor ALUs have a wire between them) - remember, in matrix
multiplication the same input (eg: a row) is used and summed over and over again.
- It makes an engineering tradeoff: limiting registers, control and operational flexibility in exchange for efficiency and much higher operation density.
Google seems to already be in the 3rd generation of TPUs.
TPU design is strictly minimal and deterministic as it has to run only one task at a time: neural network prediction/training.
TPUs are more specialized for machine learning calculations than GPUs.
1. Google designed it as a Matrix Processor that was specialized for neural network work loads.
2. Compute with Extreme Parallelism.
3. Highly-optimized for large batches and CNNs (convolutional neural network).
(https://cloud.google.com/blog/products/ai-machine-learning/an-in-depth-look-at-googles-first-tensor-processing-unit-tpu)
-- TPU vs GPU vs CPU for ML
- TPU: Tensor Processing Unit is highly-optimised for large batches and CNNs and has the highest training throughput.
- GPU: Graphics Processing Unit shows better flexibility and programmability for irregular computations, such as small batches and nonMatMul computations.
- CPU: Central Processing Unit achieves the highest FLOPS utilisation for RNNs and supports the largest model because of large memory capacity.

-- Concurrency vs Parallelism
- Concurrency is when two or more tasks can start, run, and complete in overlapping time periods.
It doesn't necessarily mean they'll ever both be running at the same instant. For example, multitasking on a single-core machine.
- Parallelism is when tasks literally run at the same time, e.g., on a multicore processor.
Concurrency is achieved through the interleaving operation of processes on the central processing unit (CPU), or in
other words by the context switching. It also increases the amount of work finished at a time.
Concurrency can have a non-deterministic control flow approach.

-- SPMD (Single Program Multiple Data)
Example: Sum two vectors A and B into C, C[i] = A[i] + B[i], iterate through it with each thread doing
the same thing (sum), but each thread over different data (different sub-range / sub-interval of A and B)
SIMD can be understood similarly as SPMD but on multiple hardware, each ALU executing the same instruction
MPI is used to perform SPMD as well (remember MPIgenerateImage on https://github.com/rogerlucena/RayTracer/blob/master/src/rt_tools.cpp).
(https://ec.europa.eu/programmes/erasmus-plus/project-result-content/6d72bb05-2d38-48d2-87e4-a209e49cda4f/lec01_intro.pdf)

-- Communication patterns on distributed computing
Brodcast (communicate to all)
Multicast (communicate to few)
Unicast (communicate to only one other)
Remember MPI example above: all other nodes unicast to 0-node (unicast), and 0-node receives from all of them and builds the image (aggregation).
There is also Brodcast, Aggregation (all-reduce too?) and Shuffle (as in map reduce, elements with same key): (https://www.researchgate.net/figure/Three-common-communication-patterns-for-distributed-applications-broadcast-aggregation_fig1_331034553).

-- Differences between programming languages
C++ faster because it does not have a garbage collector, Java and Python do.
C++ faster because static typing, Python supports dynamic typing.
C++ and Java use a compiler, Python uses an interpreter.
  (Interpreters usually take less amount of time to analyze the source code. However, the overall execution time is comparatively slower than compilers.)
  (Interpreters are line by line, Compilers take the whole program into machine code)
  https://www.geeksforgeeks.org/difference-between-compiler-and-interpreter/

-- Float numbers (understand mantissa, accuracy ...)
To better understand mantissa, exponent etc: https://www.cprogramming.com/tutorial/floating_point/understanding_floating_point_representation.html
Why floating-point values do not represent exact value: https://www.geeksforgeeks.org/why-floating-point-values-do-not-represent-exact-value/
Some numbers cannot be represented as the sum of 1/2 + 1/4 + 1/8 ..., such as  0.1: https://www.cs.technion.ac.il/users/yechiel/c++-faq/floating-point-arith.html
(Just like 1/3 in decimal base)

-- Trees (attention here):
Depth: distance (#edges) from root (d(root) = 0)
Height: largest distance (#edges) to a leaf (h(every leaf) = 0)
Height of the tree: h(root)
In a completely filled Binary Tree (all levels filled) the total number of nodes is 2^(h + 1) - 1.
If only the last level is partially filled to the left (definition of "Complete Binary Tree"), then the 
total number of nodes is at least 2^h and at most 2^(h + 1) - 1.

-- Binary Search Tree (BST):
A BST is a type of binary tree data structure in which each node contains
a __unique__ key and satisfies a specific ordering property:
a) All nodes in the left subtree of a node contain values __strictly__ less than the nodeâs value.
b) All nodes in the right subtree of a node contain values __strictly__ greater than the nodeâs value.

-- Idea to prove Greedy:
https://www.interviewbit.com/tutorial/activity-selection-problem/
If other ("k") than the greedy choice ("1") was taken, show that "(S - {k}) U {1}" would also be a solution 
(remember the example of choosing tasks, take the ones with smaller finish time first)
(or the _assign_mice_to_holes problem - sort both and remember: max(|i1 - j1|, |i2 - j2|) <= max(|i1 - j2|, |i2 - j1|))

-- Dynamic Programming
1) Try to divide into smaller recursive problems, if overlaps -> memoization
2) See space and time, try to optimize

-- Difference between Delete and Free:
Delete is an operator while Free is a function. Delete frees the allocated memory and also calls 
the destructor of the class in C++ while free() does not calls any destructor and only free the allocated 
memory set with malloc in C. Free() uses C run time heap whereas Delete may be overloaded on class basis to use private heap.
Syntax: delete ptr; free(ptr);
Delete takes a pointer and releases that memory to be used by someone else too, calling the destructor before that (if defined).
eg: Destructor for TreeNode can be {delete left; delete right;} called by Delete over a ptr (recursively call children and
by the end it automatically frees itself too, do not call for "this" pointer).

-- There is a std::swap function:
Use it to swap contents of vars.
For vectors use vector::swap (u.swap(v)) method: O(1) instead of O(n) with std:swap.
(both above work even for vectors of different sizes)
https://www.geeksforgeeks.org/difference-between-stdswap-and-stdvectorswap/

-- Remember: If "v" is an empty vector -> v.size()-1: 18446744073709551615
Attention when iterating a vector backwards, in the edge case of empty input you can be in trouble!
Treat this edge case separated (maybe along with v.size() == 1) at the beginning!
or do a "+1" operation over the index to compare, instead of "-1" over the size; or cast to int if allowed.

-- Remember: mid = left + (right - left) / 2 to avoid int overflow!

-- Graph representations:
Adjacency Matrix: 
    Pros: O(1) to see if an edge
    Cons: Consumes O(V^2) space (even if sparse), O(V^2) to add a new vertex too (if you have to 
          copy everything into a new matrix; in this case, think about amortized to improve time efficiency)
Asjacency list:
    Pros: O(V+E) in space, adding a vertex is easier; space worst case: C(V, 2) edges in a graph thus consuming O(V^2) space
    Cons: To verify if an edge, O(V) in the worst case (you can use a hash set for the neighbors to improve this)

-- Dijkstra vs BFS (vs Bellman-Ford - extra)
Use Djikstra only if weighted edges, and no negative edges!
Remember Djikstra: http://www.inf.ufsc.br/grafos/temas/custo-minimo/dijkstra.html (use wayback machine to see, or see GeeksForGeeks)
Complexity: O((V + E) log V) in time using a heap, makes sense, and O(V) in extra space (heap) - https://www.geeksforgeeks.org/dsa/time-and-space-complexity-of-dijkstras-algorithm
Otherwise, a simple BFS will do the work.
Dijkstra fails with negative edges because it assumes that once a nodeâs shortest path is found (node is "closed", greedy local assumption that leads to global 
optimum), it cannot be improved with any of the other farther nodes in the min-heap (which is true for the case with non-negative edges).
For the case with negative edges, think of using Bellman-Ford (pretty straight-forward):
    Do the relaxation of all edges V-1 times (size of the longest shortest path without negative cycles), where for each edge [u, v]
    relaxation means comparing distance[v] > distance[u] + w, if yes update distance[v] and sets u as the predecessor[v].
    Complexity: O(V*E) in time (two fors, one for V-1 and an inner one over all edges E) - normally slower than Dijkstra, and O(V) in space.
    To see if there is a negative cycle, do one more time and if any edge is still relaxed we have a negative cycle.
    (Reference https://www.geeksforgeeks.org/dsa/bellman-ford-algorithm-dp-23 and ChatGPT)
        Note: remember "tmp_dist = dist" before the inner "for" over edges, update only "tmp_dist", and then 
        assign "dist = tmp_dist" after for next loop over V-1 - see __cheapest_flights_within_k_stops.cpp

-- Difference between "set/multiset" and "priority queues":
Inside "remember_set_and_map.cpp"
You can use a set as a pq too for nice update in value, see "Data structure bizuradas menos macaco" above.

-- For maps and sets we cannot do operations with the iterators such as:
"m.upper_bound(n) - m.begin()", or "m.begin()+1" -> bidirectional iterators do not allow it, only ++ or -- 
"set/map/multiset" are normally implemented over binary search trees (AVL or RB trees)
"priority_queues" are implemented over balanced heaps (!= than BSTs)

-- Difference between "list" and "deque"
"deque" uses a dynamic array, "list" is a doubly linked list
"deque" provides random acess and has almost same interface as a vector
"list" fast insertion and deletion in any position, "deque" only in end/beginning (middle is linear)
"deque" with insert or delete in any position other than end/beginning invalidates all pointers/iterators/references 
    to elements of it, differently than "list", see lru_cache problem
https://stackoverflow.com/questions/1436020/whats-the-difference-between-deque-and-list-stl-containers
see remember_deque_and_list file

-- SQL vs NoSQL
SQL is for structured data not changing with time, has a wide support, vendors, mature techlogy, better for complex queries,
    faster for joins, relations between datapoints are more relevant (table based - MySQL, PostgreSQL). Vertically scalable.
    specialized to serve business operations, financial transactions, asset management and personal data management.  
    SQL systems best support structured data requiring precision.  Historically designed and optimized to assure transaction consistency.
NoSQL is open-source, relies on the community, better for big data applications, more scalable, low-level languages,
    faster write/read 1 datapoint (graphs, documents based - MongoDB, columns based, key values - Redis). Horizontally scalable.
    Specialized to support massively parallel and geographically distributed database systems, such as web applications.
https://www.geeksforgeeks.org/sql-vs-nosql-which-one-is-better-to-use/?ref=rp

-- Interesting:
- Perform a modulus operation with negative numbers -> (a % b + b) % b (eg in string_shifts problem)

-- Circular arrays (Soldado mentioned this pattern for problems in his doc):
Forward wrap: (i + k) % n
Backward wrap: (i - k + n) % n (to avoid negative indices)
- If sliding window with wrap around, it can be useful to double the array (eg: to sum k consecutive elements):
vector<int> extended(arr.begin(), arr.end());
extended.insert(extended.end(), arr.begin(), arr.end());  // and then indexes are linearized now.
Extra memory but simpler logic (acceptable in interviews unless problem says "O(1) extra space") - using modulus if possible can avoid that too but easier to make mistakes in logic.
- Other example - next greater element in circular array (use a monotonic stack and iterate twice over the array to simulate circularity):
for(int i = 2 * n - 1; i >= 0; --i) {
    while(!st.empty() && arr[i % n] >= st.top()) {
        st.pop();
    }
    next_greater[i % n] = st.empty() ? -1 : st.top();
    st.push(arr[i % n]);
}

-- Patterns for coding interviews (like grokking):
https://hackernoon.com/14-patterns-to-ace-any-coding-interview-question-c5bb3357f6ed 
https://dev.to/arslan_ah/the-ultimate-strategy-to-preparing-for-the-coding-interview-3ace

### impt ###

-- Dicas Carolzinha e outras (top 3 coisas pra estudar)
- Linked Lists (70% das questÃµes dela)
- Percurso em grafo, DFS/BFS (dijkstra, minimum path, red-black & AVL trees - nÃ£o necessÃ¡rio)
- RecursÃ£o, DP

Dicas forum: graph questions and string manipulations are common.

- coisas implementaÃ§Ã£o by heart: inverter uma linked list (https://www.geeksforgeeks.org/reverse-a-linked-list/), 
         binary search, merge two sorted linked lists (in remember_liked_lists.cpp), nÃ£o pembe em DFS/BFS! (code imediato!),
        topological ordering, quick sort (?), rever outros sorts tb (merge, insertion, bubble, radix, bucket, counting ...)
- revisa bem: hash_map (cai muito!), queues, stacks (problema dos parÃªnteses, ver "merge_overlapping_intervals.cpp" tambÃ©m)
- knapsack problem, traveling salesman, "coin_exchange.cpp", "get_subsets.cpp" - DP
- remember classes, inheritance, abstract, virtual (geeksforgeeks)

-- Lembrar, passo a passo (do not speak too fast!!):
1) Faz um exemplo simples pra ver se realmente entendeu, se necessÃ¡rio, e jÃ¡ mencione corner cases aqui tbm
    Ask clarification questions to see if you really got what they want (float numbers? sorted? 
    repeated numbers? negative? can I get two of the same index? Any time/space requirements?
    se nao tem resposta, o que retorno nesse caso? (eg: era empty no caso de {1} no pb do produto dos outros),
    vetor ordenado? lowercase?)
2) Always think out loud! 
    se precisar de 1 min em silÃªncio, PEÃ§A! Comunique isso!
    problema aberto? pede a API, pensa no caso mais simple possÃ­vel (squared room with discret cells), se 
    errado ele te corrige
3) Brute-force and complexities (importante falar o brute-force logo! - nada bom se ele achar que nem o brute vc sabe) 
    then optimze! (no coding yet!)
    REMEMBER: start with the brute force approach and then check if you can do better :) 
    (if you start to optimize too early that can hurt you! as it happened in the award_budget_cuts problem)
    SE VC estah pensando em uma estrategia, mas viu um possivel problema, FALE, ele pode te dizer como
    resolver (eg: heaps tamanhos mto diferents no pb da median de um stream), ou que nem eh possivel ter esse problema
4) Should I optimize more or can I code? Think out loud while coding!
    Tudo eh ok, desde que vc fale. âNormalmente essa var deveria ter nome tal .. mas for the sake of time/simplicity, letâs use ..â
    funcÃ£o auxiliar que as vezes nem precisa codar no final! 
    remember, if repeated code, try to simplify with auxiliar vector and "for"! Exemplo below (frog and river problem):
        vector<int> new_speeds = {speed - 1, speed, speed + 1};
        for (int new_speed : new_speeds) {
	            int new_pos = pos + new_speed; ...
5) Test it line by line (do not skip lines)! Colocando valores das variÃ¡veis ao lado como comentÃ¡rio
    Comeca com os mais simples (empty, {1}, corner cases, then small ones - 2/3 els, jÃ¡ Ã© enough pra achar bugs)! 2x2 ja dÃ¡ (robot)
    nao pega o input do problema original se muito grande se nao muito tempo testando!
        tempo valioso para follow ups ou outros problemas! simplifica o input para menor e roda nele se quiser (lembra feedback do pramp - careca irwin)
6) No final: ask questions about the work there, iteresting projects ...


- Red flags (pega mal): base case recursion, lembre usar a condition tb otherwise infinite loop! 
    Nao inicializar alguma variavel, DFS, BFS, update step for not infinity loop inside while,
    usar constantes literais em vez de dar um nome meaningful pra elas (leftEdge em vez de -1 por eg)
- Invariants â¦ boa palavra, gostam quando usamos (e.g.: you have to phisically bring the robot back) 

se em algum momento viu um edge case:
    pode comentar em voz alta, colocar um comentario pra lidar depois, ou se simples resolver logo
    pode falar de edges cases em voz alta na analise do algo mesmo antes de codar tb

here -> remember_by_heart.cpp -> files with __ -> googles docs dicas Soldado (my version too with more questions to ask the interviewer there, and team match tips as well)

Think about inverting, calculating, and then inverting again:
    invert_words Pramp problem (reverse_the_string problem in ./Problems) ...
Remember std::to_string() method and s.append() in the place of ostringstream (remember_strings.cpp file), for example

Ideia geral para DP:
    if (caso base)
        return resposta
    if (nao tah no cache)
        calcula e bota no cache
    return valor do cache

Ideia geral para implementar iterators:
    constructor
        state = algo (inicializacao)
        update
    next
        ans = elements[state]
        update
        return ans
    has_next
        verificar state 

Macaco: anagramas, pense em fingerprint da string
    a1b2
    sort na string
    vetor com 26 posicoes e o count de cada char

Cuidado com saltos logicos, se vc tah assumindo uma DS jah, diga isso
    e.g. to assumindo que as palavras do ingles ja tao num hash set (na hora de mencionar complexidade com O(1) access)

Sempre pensar se dÃ¡ pra fazer um preprocessing que ajude
    aqui seria ja organizar num hash map de fingerprint/signature para vector<string>

C++ (lembrar)
    lembrar que [] no map modifica, tem que usar at() se vocÃª recebeu o hash map por const ref (read-only access)
    ele default inicializa/insert com 0 se o value do hash map for int
